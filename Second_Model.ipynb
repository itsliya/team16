{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (1.21.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycler==0.10.0\r\n",
      "kiwisolver==1.3.2\r\n",
      "matplotlib==3.4.3\r\n",
      "numpy==1.21.3\r\n",
      "pandas==1.4.2\r\n",
      "Pillow==8.4.0\r\n",
      "pyparsing==3.0.1\r\n",
      "python-dateutil==2.8.2\r\n",
      "pytz==2021.3\r\n",
      "scipy==1.7.1\r\n",
      "seaborn==0.11.2\r\n",
      "six==1.16.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b1905f3357c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns= [\n",
    "        'NAME_EDUCATION_TYPE', \n",
    "        'NAME_FAMILY_STATUS', \n",
    "        'NAME_HOUSING_TYPE',\n",
    "        'DAYS_BIRTH',\n",
    "        'OWN_CAR_AGE',\n",
    "        'FLAG_EMP_PHONE',\n",
    "        'FLAG_WORK_PHONE',\n",
    "        'FLAG_CONT_MOBILE',\n",
    "        'FLAG_PHONE',\n",
    "        'FLAG_EMAIL',\n",
    "        'CNT_FAM_MEMBERS',\n",
    "        'REGION_RATING_CLIENT',\n",
    "        'WEEKDAY_APPR_PROCESS_START',\n",
    "        'HOUR_APPR_PROCESS_START',\n",
    "        'APARTMENTS_AVG',\n",
    "        'BASEMENTAREA_AVG',\n",
    "        'YEARS_BEGINEXPLUATATION_AVG',\n",
    "        'YEARS_BUILD_AVG',\n",
    "        'COMMONAREA_AVG',\n",
    "        'ELEVATORS_AVG',\n",
    "        'ENTRANCES_AVG',\n",
    "        'FLOORSMAX_AVG',\n",
    "        'FLOORSMIN_AVG',\n",
    "        'LANDAREA_AVG',\n",
    "        'LIVINGAPARTMENTS_AVG',\n",
    "        'LIVINGAREA_AVG',\n",
    "        'NONLIVINGAPARTMENTS_AVG',\n",
    "        'NONLIVINGAREA_AVG',\n",
    "        'APARTMENTS_MODE',\n",
    "        'BASEMENTAREA_MODE',\n",
    "        'YEARS_BEGINEXPLUATATION_MODE',\n",
    "        'YEARS_BUILD_MODE',\n",
    "        'COMMONAREA_MODE',\n",
    "        'ELEVATORS_MODE',\n",
    "        'ENTRANCES_MODE',\n",
    "        'FLOORSMAX_MODE',\n",
    "        'FLOORSMIN_MODE',\n",
    "        'LANDAREA_MODE',\n",
    "        'LIVINGAPARTMENTS_MODE',\n",
    "        'LIVINGAREA_MODE',\n",
    "        'NONLIVINGAPARTMENTS_MODE',\n",
    "        'NONLIVINGAREA_MODE',\n",
    "        'APARTMENTS_MEDI',\n",
    "        'BASEMENTAREA_MEDI',\n",
    "        'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "        'YEARS_BUILD_MEDI',\n",
    "        'COMMONAREA_MEDI',\n",
    "        'ELEVATORS_MEDI',\n",
    "        'ENTRANCES_MEDI',\n",
    "        'FLOORSMAX_MEDI',\n",
    "        'FLOORSMIN_MEDI',\n",
    "        'LANDAREA_MEDI',\n",
    "        'LIVINGAPARTMENTS_MEDI',\n",
    "        'LIVINGAREA_MEDI',\n",
    "        'NONLIVINGAPARTMENTS_MEDI',\n",
    "        'NONLIVINGAREA_MEDI',\n",
    "        'FONDKAPREMONT_MODE',\n",
    "        'HOUSETYPE_MODE',\n",
    "        'TOTALAREA_MODE',\n",
    "        'WALLSMATERIAL_MODE',\n",
    "        'EMERGENCYSTATE_MODE',\n",
    "        'DAYS_LAST_PHONE_CHANGE',\n",
    "        'FLAG_DOCUMENT_2',\n",
    "        'FLAG_DOCUMENT_3',\n",
    "        'FLAG_DOCUMENT_4',\n",
    "        'FLAG_DOCUMENT_5',\n",
    "        'FLAG_DOCUMENT_6',\n",
    "        'FLAG_DOCUMENT_7',\n",
    "        'FLAG_DOCUMENT_8',\n",
    "        'FLAG_DOCUMENT_9',\n",
    "        'FLAG_DOCUMENT_10',\n",
    "        'FLAG_DOCUMENT_11',\n",
    "        'FLAG_DOCUMENT_12',\n",
    "        'FLAG_DOCUMENT_13',\n",
    "        'FLAG_DOCUMENT_14',\n",
    "        'FLAG_DOCUMENT_15',\n",
    "        'FLAG_DOCUMENT_16',\n",
    "        'FLAG_DOCUMENT_17',\n",
    "        'FLAG_DOCUMENT_18',\n",
    "        'FLAG_DOCUMENT_19',\n",
    "        'FLAG_DOCUMENT_20',\n",
    "        'FLAG_DOCUMENT_21'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to focus only on the columns above. Repeat steps above for the other data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('bureau_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv('credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv('installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv('POS_CASH_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the data based on loan ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 = df1.merge(df2, left_on='SK_ID_CURR', right_on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2_3 = df1_2.merge(df3, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_4 = df1.merge(df4, left_on='SK_ID_CURR', right_on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_4_5 = df1.merge(df5, left_on='SK_ID_PREV', right_on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_4_5_6 = df1.merge(df6, left_on='SK_ID_PREV', right_on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_4_5_6_7 = df1.merge(df7, left_on='SK_ID_PREV', right_on='SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     IMPORTANT: IGNORE CELLS BELOW!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create dummy variables with the remaining categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_val = {'10+ years':10, '2 years': 2, '3 years': 3, \n",
    "          '< 1 year': 0.5, '1 year': 1, '5 years': 5,\n",
    "          '4 years': 4, '6 years': 6, '8 years': 8, '7 years': 7, '9 years': 9}\n",
    "\n",
    "df['emp_length'] = df.emp_length.map(len_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_values = {' 36 months': 36, ' 60 months': 60}\n",
    "df['term'] = df.term.map(term_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print([column for column in df.columns if df[column].dtype == object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = (\n",
    "    ['sub_grade', 'home_ownership', 'verification_status', 'purpose', \n",
    "      'initial_list_status', 'application_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=dummies, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be numerical so we need to map a binary outcome as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = {'Fully Paid': 1, 'Charged Off': 0}\n",
    "df['loan_status'] = df.loan_status.map(loan)\n",
    "df['loan_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin the machine learning models. First, we need to split the training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report, \n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    plot_confusion_matrix, plot_roc_curve\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check if the dataset is balanced or not in order to know what kind of metrics are we going to use to assess the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p = df.loan_status.value_counts()[0] / df.shape[0]\n",
    "w_n = df.loan_status.value_counts()[1] / df.shape[0]\n",
    "\n",
    "print(f\"Weight of positive values {w_p}\")\n",
    "print(f\"Weight of negative values {w_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed()\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loan_status.value_counts()[1] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outiliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers on the training set could really negatively impact our model since it could create biais. We are going to deal with them in the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train['dti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dti'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the variable 'dti' has a lot of outliers. We could remove all the values that are above 50. \n",
    "\n",
    "We can do the same kind of work on the remaining numerical variables and we can remove their respectively ouliers. Finally, we can keep the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['annual_inc'] <= 250000]\n",
    "train = train[train['dti'] <= 50]\n",
    "train = train[train['open_acc'] <= 40]\n",
    "train = train[train['total_acc'] <= 80]\n",
    "train = train[train['revol_util'] <= 120]\n",
    "train = train[train['revol_bal'] <= 250000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the training and testing set from the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop('loan_status', axis=1), train.loan_status\n",
    "X_test, y_test = test.drop('loan_status', axis=1), test.loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we implement is the logistic regression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=88, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the confusion matrix\n",
    "y_prob = logreg.predict_proba(X_test)\n",
    "y_pred_log = pd.Series([1 if x > 0.5 else 0 for x in y_prob[:,1]], index=y_test.index)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_log)\n",
    "print (\"Confusion Matrix: \\n\", cm)\n",
    "print (\"\\nPrecision:\", precision_score(y_test, y_pred_log))\n",
    "print (\"\\nF1:\", f1_score(y_test, y_pred_log))\n",
    "print (\"\\nauc:\", roc_auc_score(y_test, y_pred_log))\n",
    "print (\"\\nrecall:\", recall_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is imbalanced, we should focus on F1 score. We obtain a good score with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use Random Forest to try to get a better F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (\"Confusion Matrix: \\n\", cm)\n",
    "print (\"\\nPrecision:\", precision_score(y_test, y_pred))\n",
    "print (\"\\nF1:\", f1_score(y_test, y_pred))\n",
    "print (\"\\nauc:\", roc_auc_score(y_test, y_pred))\n",
    "print (\"\\nrecall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test.iloc[0:1])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
